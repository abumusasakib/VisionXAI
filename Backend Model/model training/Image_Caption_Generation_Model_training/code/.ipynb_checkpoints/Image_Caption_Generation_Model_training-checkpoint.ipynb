{"cells":[{"cell_type":"markdown","metadata":{"id":"FVcpHBC7IgqH"},"source":"# Preliminary settings"},{"cell_type":"markdown","metadata":{"id":"HF4SBBAFIsOB"},"source":"Libraries imported."},{"cell_type":"code","execution_count":null,"metadata":{"id":"nxWhGVY-EN5v"},"outputs":[],"source":"import os\nimport re\nimport json\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.applications import efficientnet\nfrom tensorflow.keras.layers import TextVectorization\n\nfrom datetime import datetime\nseed = int(round(datetime.now().timestamp()))\nnp.random.seed(seed)\ntf.random.set_seed(seed)"},{"cell_type":"markdown","metadata":{"id":"TSIzi06ZJFbP"},"source":"Path to data."},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZMofSLZmE2RB"},"outputs":[],"source":"dx = \"/data/train/rxxch9vw59.2/\""},{"cell_type":"markdown","metadata":{"id":"AWc1185pJOS9"},"source":"Important constants."},{"cell_type":"code","execution_count":null,"metadata":{"id":"TDp1iVL5EN5x"},"outputs":[],"source":"# Path to the images\nIMAGES_PATH = dx+\"images\"\n\n# Desired image dimensions\nIMAGE_SIZE = (499, 499)\n\n# Vocabulary size\nVOCAB_SIZE = 75000\n\n# Fixed length allowed for any sequence\nSEQ_LENGTH = 25\n\n# Dimension for the image embeddings and token embeddings\nEMBED_DIM = 1024\n\n# Per-layer units in the feed-forward network\nFF_DIM = 1024\n\n# Other training parameters\nBATCH_SIZE = 64\nEPOCHS = 1\nAUTOTUNE = tf.data.AUTOTUNE"},{"cell_type":"markdown","metadata":{"id":"dWXtvf3cJxTI"},"source":"# Dataset creation"},{"cell_type":"markdown","metadata":{"id":"_ZFNKop3SEKA"},"source":"The image files are loaded. Each image is paired with a caption.\nThe pairs are shuffled and split into 20% test and 80% train set."},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"do7RDmN_EN5x","outputId":"52dd409a-4830-43bd-c303-e24f8660da50"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of training samples:  14154\n","Number of validation samples:  3539\n"]}],"source":"def load_captions_data(filename):\n    \"\"\"Loads captions (text) data and maps them to corresponding images.\n\n    Args:\n        filename: Path to the text file containing caption data.\n\n    Returns:\n        caption_mapping: Dictionary mapping image names and the corresponding captions\n        text_data: List containing all the available captions\n    \"\"\"\n    with open(filename, encoding=\"utf8\") as caption_file:\n        caption_data = json.load(caption_file)\n        caption_mapping = {}\n        text_data = []\n\n        for item in caption_data:  # Iterate over the list of dictionaries\n            img_name = os.path.join(IMAGES_PATH, item['filename'].strip())  # Access 'filename'\n            caption_mapping[img_name] = [\"<start> \" + caption.strip() + \" <end>\" for caption in item['caption']]  # Access 'caption'\n            text_data.extend(caption_mapping[img_name])\n\n        return caption_mapping, text_data\n\n\ndef train_val_split(caption_data, train_size=0.8, shuffle=True):\n    \"\"\"Split the captioning dataset into train and validation sets.\n\n    Args:\n        caption_data (dict): Dictionary containing the mapped caption data\n        train_size (float): Fraction of all the full dataset to use as training data\n        shuffle (bool): Whether to shuffle the dataset before splitting\n\n    Returns:\n        Traning and validation datasets as two separated dicts\n    \"\"\"\n\n    # 1. Get the list of all image names\n    all_images = list(caption_data.keys())\n\n    # 2. Shuffle if necessary\n    if shuffle:\n        np.random.shuffle(all_images)\n\n    # 3. Split into training and validation sets\n    train_size = int(len(caption_data) * train_size)\n\n    training_data = {\n        img_name: caption_data[img_name] for img_name in all_images[:train_size]\n    }\n    validation_data = {\n        img_name: caption_data[img_name] for img_name in all_images[train_size:]\n    }\n\n    # 4. Return the splits\n    return training_data, validation_data\n\n\n# Load the dataset\ncaptions_mapping, text_data = load_captions_data(dx + \"captions.json\")\n\n# Split the dataset into training and validation sets\ntrain_data, valid_data = train_val_split(captions_mapping)\nprint(\"Number of training samples: \", len(train_data))\nprint(\"Number of validation samples: \", len(valid_data))"},{"cell_type":"markdown","metadata":{"id":"uq7RaW9-LC9b"},"source":"Image augmentation is performed and the captions are vectorised."},{"cell_type":"code","execution_count":null,"metadata":{"id":"1UFMzUbMEN5z"},"outputs":[],"source":"def custom_standardization(input_string):\n    lowercase = tf.strings.lower(input_string)\n    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n\n\nstrip_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\nstrip_chars = strip_chars.replace(\"<\", \"\")\nstrip_chars = strip_chars.replace(\">\", \"\")\n\nvectorization = TextVectorization(\n    max_tokens=VOCAB_SIZE,\n    output_mode=\"int\",\n    output_sequence_length=SEQ_LENGTH,\n    standardize=custom_standardization,\n)\nvectorization.adapt(text_data)\n\n# Data augmentation for image data\nimage_augmentation = keras.Sequential(\n    [\n        layers.RandomFlip(\"horizontal\"),\n        layers.RandomRotation(0.2),\n        layers.RandomContrast(0.3),\n    ]\n)\n"},{"cell_type":"markdown","metadata":{"id":"ENuangz2LOaT"},"source":"The train and test datasets are created using TensorFlow Dataset."},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eOUvwcMeEN50","outputId":"6ae87204-37bb-46d8-cef5-24ad77ef350b"},"outputs":[{"name":"stdout","output_type":"stream","text":["[83, 83, 71, 65, 65]\n","[5, 5, 5, 5, 5]\n","[83, 83, 66, 82, 83]\n","[5, 5, 5, 5, 5]\n"]}],"source":"def decode_and_resize(img_path):\n    img = tf.io.read_file(img_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, IMAGE_SIZE)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    return img\n\n\ndef process_input(img_path, captions):\n    return decode_and_resize(img_path), vectorization(captions)\n\n\ndef make_dataset(images, captions):\n    dataset = tf.data.Dataset.from_tensor_slices((images, captions))\n    dataset = dataset.shuffle(BATCH_SIZE * 8)\n    dataset = dataset.map(process_input, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n\n    return dataset\n\n# def make_dataset(images, captions):\n#     print([len(i) for i in images[:5]])\n#     print([len(i) for i in captions[:5]])\n\n\n# Pass the list of images and the list of corresponding captions\ntrain_dataset = make_dataset(list(train_data.keys()), list(train_data.values()))\n\nvalid_dataset = make_dataset(list(valid_data.keys()), list(valid_data.values()))\n"},{"cell_type":"markdown","metadata":{"id":"u5j57Tz3Mzpi"},"source":"# Transformer model creation"},{"cell_type":"markdown","metadata":{"id":"9aMSR1TdSMJK"},"source":"The Transformer uses the EfficientNetB0 CNN.\nThe Encoder and Decoder block and Positional Embedding layer is seperately created by inheriting the Layer class.\nThe Transformer model is created by compiling the layers and inheriting the Model class."},{"cell_type":"code","execution_count":null,"metadata":{"id":"xDNvmXFSEN50","jupyter":{"source_hidden":false}},"outputs":[],"source":"def get_cnn_model():\n    base_model = efficientnet.EfficientNetB0(\n        input_shape=(*IMAGE_SIZE, 3), include_top=False, weights=\"imagenet\",\n    )\n    # We freeze our feature extractor\n    base_model.trainable = False\n    base_model_out = base_model.output\n    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)\n    cnn_model = keras.models.Model(base_model.input, base_model_out)\n    return cnn_model\n\n\nclass TransformerEncoderBlock(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n        self.attention_1 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim, dropout=0.0\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.dense_1 = layers.Dense(embed_dim, activation=\"relu\")\n\n    def call(self, inputs, training, mask=None):\n        inputs = self.layernorm_1(inputs)\n        inputs = self.dense_1(inputs)\n\n        attention_output_1 = self.attention_1(\n            query=inputs,\n            value=inputs,\n            key=inputs,\n            attention_mask=None,\n            training=training,\n        )\n        out_1 = self.layernorm_2(inputs + attention_output_1)\n        return out_1\n\n\nclass PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(\n            input_dim=vocab_size, output_dim=embed_dim\n        )\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=embed_dim\n        )\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_tokens = embedded_tokens * self.embed_scale\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)\n\n\nclass TransformerDecoderBlock(layers.Layer):\n    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.ff_dim = ff_dim\n        self.num_heads = num_heads\n        self.attention_1 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n        )\n        self.attention_2 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n        )\n        self.ffn_layer_1 = layers.Dense(ff_dim, activation=\"relu\")\n        self.ffn_layer_2 = layers.Dense(embed_dim)\n\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.layernorm_3 = layers.LayerNormalization()\n\n        self.embedding = PositionalEmbedding(\n            embed_dim=EMBED_DIM, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE\n        )\n        self.out = layers.Dense(VOCAB_SIZE, activation=\"softmax\")\n\n        self.dropout_1 = layers.Dropout(0.3)\n        self.dropout_2 = layers.Dropout(0.5)\n        self.supports_masking = True\n\n    def call(self, inputs, encoder_outputs, training, mask=None):\n        inputs = self.embedding(inputs)\n        causal_mask = self.get_causal_attention_mask(inputs)\n\n        if mask is not None:\n            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n            combined_mask = tf.minimum(combined_mask, causal_mask)\n\n        attention_output_1 = self.attention_1(\n            query=inputs,\n            value=inputs,\n            key=inputs,\n            attention_mask=combined_mask,\n            training=training,\n        )\n        out_1 = self.layernorm_1(inputs + attention_output_1)\n\n        attention_output_2 = self.attention_2(\n            query=out_1,\n            value=encoder_outputs,\n            key=encoder_outputs,\n            attention_mask=padding_mask,\n            training=training,\n        )\n        out_2 = self.layernorm_2(out_1 + attention_output_2)\n\n        ffn_out = self.ffn_layer_1(out_2)\n        ffn_out = self.dropout_1(ffn_out, training=training)\n        ffn_out = self.ffn_layer_2(ffn_out)\n\n        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)\n        ffn_out = self.dropout_2(ffn_out, training=training)\n        preds = self.out(ffn_out)\n        return preds\n\n    def get_causal_attention_mask(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size, sequence_length = input_shape[0], input_shape[1]\n        i = tf.range(sequence_length)[:, tf.newaxis]\n        j = tf.range(sequence_length)\n        mask = tf.cast(i >= j, dtype=\"int32\")\n        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n        mult = tf.concat(\n            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n            axis=0,\n        )\n        return tf.tile(mask, mult)\n\n\nclass ImageCaptioningModel(keras.Model):\n    def __init__(\n        self, cnn_model, encoder, decoder, num_captions_per_image=5, image_aug=None,\n    ):\n        super().__init__()\n        self.cnn_model = cnn_model\n        self.encoder = encoder\n        self.decoder = decoder\n        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n        self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n        self.num_captions_per_image = num_captions_per_image\n        self.image_aug = image_aug\n\n    def calculate_loss(self, y_true, y_pred, mask):\n        loss = self.loss(y_true, y_pred)\n        mask = tf.cast(mask, dtype=loss.dtype)\n        loss *= mask\n        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n\n    def calculate_accuracy(self, y_true, y_pred, mask):\n        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n        accuracy = tf.math.logical_and(mask, accuracy)\n        accuracy = tf.cast(accuracy, dtype=tf.float32)\n        mask = tf.cast(mask, dtype=tf.float32)\n        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n\n    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):\n        encoder_out = self.encoder(img_embed, training=training)\n        batch_seq_inp = batch_seq[:, :-1]\n        batch_seq_true = batch_seq[:, 1:]\n        mask = tf.math.not_equal(batch_seq_true, 0)\n        batch_seq_pred = self.decoder(\n            batch_seq_inp, encoder_out, training=training, mask=mask\n        )\n        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)\n        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)\n        return loss, acc\n\n    def train_step(self, batch_data):\n        batch_img, batch_seq = batch_data\n        batch_loss = 0\n        batch_acc = 0\n\n        if self.image_aug:\n            batch_img = self.image_aug(batch_img)\n\n        # 1. Get image embeddings\n        img_embed = self.cnn_model(batch_img)\n\n        # 2. Pass each of the captions one by one to the decoder\n        # along with the encoder outputs and compute the loss as well as accuracy\n        # for each caption.\n        for i in range(self.num_captions_per_image):\n            with tf.GradientTape() as tape:\n                loss, acc = self._compute_caption_loss_and_acc(\n                    img_embed, batch_seq[:, i, :], training=True\n                )\n\n                # 3. Update loss and accuracy\n                batch_loss += loss\n                batch_acc += acc\n\n            # 4. Get the list of all the trainable weights\n            train_vars = (\n                self.encoder.trainable_variables + self.decoder.trainable_variables\n            )\n\n            # 5. Get the gradients\n            grads = tape.gradient(loss, train_vars)\n\n            # 6. Update the trainable weights\n            self.optimizer.apply_gradients(zip(grads, train_vars))\n\n        # 7. Update the trackers\n        batch_acc /= float(self.num_captions_per_image)\n        self.loss_tracker.update_state(batch_loss)\n        self.acc_tracker.update_state(batch_acc)\n\n        # 8. Return the loss and accuracy values\n        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n\n    def test_step(self, batch_data):\n        batch_img, batch_seq = batch_data\n        batch_loss = 0\n        batch_acc = 0\n\n        # 1. Get image embeddings\n        img_embed = self.cnn_model(batch_img)\n\n        # 2. Pass each of the five captions one by one to the decoder\n        # along with the encoder outputs and compute the loss as well as accuracy\n        # for each caption.\n        for i in range(self.num_captions_per_image):\n            loss, acc = self._compute_caption_loss_and_acc(\n                img_embed, batch_seq[:, i, :], training=False\n            )\n\n            # 3. Update batch loss and batch accuracy\n            batch_loss += loss\n            batch_acc += acc\n\n        batch_acc /= float(self.num_captions_per_image)\n\n        # 4. Update the trackers\n        self.loss_tracker.update_state(batch_loss)\n        self.acc_tracker.update_state(batch_acc)\n\n        # 5. Return the loss and accuracy values\n        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n\n    @property\n    def metrics(self):\n        # We need to list our metrics here so the `reset_states()` can be\n        # called automatically.\n        return [self.loss_tracker, self.acc_tracker]\n\n\ncnn_model = get_cnn_model()\nencoder = TransformerEncoderBlock(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=1)\ndecoder = TransformerDecoderBlock(embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=2)\ncaption_model = ImageCaptioningModel(\n    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,\n)"},{"cell_type":"markdown","metadata":{"id":"hV7XUwZIPLKa"},"source":"The loss function and early stopping is defined. The model is compiled with the same."},{"cell_type":"code","execution_count":null,"metadata":{"id":"1vztqwkOEN51"},"outputs":[],"source":"# Define the loss function\ncross_entropy = keras.losses.SparseCategoricalCrossentropy(\n    from_logits=False, reduction=\"none\"\n)\n\n# EarlyStopping criteria\nearly_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n\n\n# Learning Rate Scheduler for the optimizer\nclass LRSchedule(keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, post_warmup_learning_rate, warmup_steps):\n        super().__init__()\n        self.post_warmup_learning_rate = post_warmup_learning_rate\n        self.warmup_steps = warmup_steps\n\n    def __call__(self, step):\n        global_step = tf.cast(step, tf.float32)\n        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n        warmup_progress = global_step / warmup_steps\n        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress\n        return tf.cond(\n            global_step < warmup_steps,\n            lambda: warmup_learning_rate,\n            lambda: self.post_warmup_learning_rate,\n        )\n\n\n# Create a learning rate schedule\nnum_train_steps = len(train_dataset) * EPOCHS\nnum_warmup_steps = num_train_steps // 15\nlr_schedule = LRSchedule(post_warmup_learning_rate=1e-4, warmup_steps=num_warmup_steps)\n\n# Compile the model\ncaption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)"},{"cell_type":"markdown","metadata":{"id":"HklGIq3dPi2W"},"source":"# Model training and testing"},{"cell_type":"markdown","metadata":{"id":"RmJ0CodqPwQJ"},"source":"Sets the version and checks if the model has been previously trained or being trained for the first time. If saved weights are found, they're loaded."},{"cell_type":"code","execution_count":null,"metadata":{"id":"NuvkTjn3d9nC"},"outputs":[],"source":"import os\n\nmdx = '231005' # Sets the version\ntmpx = f'/data/Model_weights/{mdx}/Temp/'\n\n# Check if the directory exists\nif not os.path.exists(tmpx):\n    print(f\"Directory {tmpx} does not exist. Creating the directory.\")\n    os.makedirs(tmpx)  # Create the directory if it doesn't exist\n\n# Now check for the files\ntry:\n    fls = os.listdir(tmpx)\n\n    if len(fls) > 0:\n        caption_model.load_weights(f'{tmpx}imgcap_{mdx}')\n        print(\"Saved weights loaded\")\n    else:\n        print(\"No saved weights found\")\n\nexcept FileNotFoundError as e:\n    print(f\"Error: {e}\")\n"},{"cell_type":"markdown","metadata":{"id":"aGNoQWakQPVn"},"source":"Lookup dictionary created and output sequence length is set."},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Jv4pzQqEN51"},"outputs":[],"source":"vocab = vectorization.get_vocabulary()\nindex_lookup = dict(zip(range(len(vocab)), vocab))\nmax_decoded_sentence_length = SEQ_LENGTH - 1\nvalid_images = list(valid_data.keys())"},{"cell_type":"markdown","metadata":{"id":"cFil9RZiQf5F"},"source":"The function reads an image from the given path. It uses the image to generate an caption. The same are displayed."},{"cell_type":"code","execution_count":null,"metadata":{"id":"SFfLGgVEmzHb","jupyter":{"source_hidden":false}},"outputs":[],"source":"def generate_caption(dt, ix):\n    # Select a random image from the validation dataset\n    sample_img = f'{dt}images/{ix}'\n\n    # Read the image from the disk\n    sample_img = decode_and_resize(sample_img)\n    img = sample_img.numpy().clip(0, 255).astype(np.uint8)\n    plt.axis('off')\n    plt.imshow(img)\n    plt.show()\n\n    # Pass the image to the CNN\n    img = tf.expand_dims(sample_img, 0)\n    img = caption_model.cnn_model(img)\n\n    # Pass the image features to the Transformer encoder\n    encoded_img = caption_model.encoder(img, training=False)\n\n    # Generate the caption using the Transformer decoder\n    decoded_caption = \"<start> \"\n    for i in range(max_decoded_sentence_length):\n        tokenized_caption = vectorization([decoded_caption])[:, :-1]\n        mask = tf.math.not_equal(tokenized_caption, 0)\n        predictions = caption_model.decoder(\n            tokenized_caption, encoded_img, training=False, mask=mask\n        )\n        sampled_token_index = np.argmax(predictions[0, i, :])\n        sampled_token = index_lookup[sampled_token_index]\n        if sampled_token == \"<end>\":\n            break\n        decoded_caption += \" \" + sampled_token\n\n    decoded_caption = decoded_caption.replace(\"<start> \", \"\")\n    decoded_caption = decoded_caption.replace(\" <end>\", \"\").strip()\n    print(\"\\nPredicted Caption: \", decoded_caption)\n    print()"},{"cell_type":"markdown","metadata":{"id":"b49GQfkuQwx6"},"source":"Test image (outside the test dataset) path."},{"cell_type":"code","execution_count":null,"metadata":{"id":"u1_T-wHXnbHw"},"outputs":[],"source":"img_dt = \"/data/test/rxxch9vw59-2/\"\nimgs = os.listdir(img_dt+\"images\")"},{"cell_type":"markdown","metadata":{"id":"2_hiYHq-Q3f-"},"source":"Generates a caption for a random image among test images."},{"cell_type":"code","execution_count":null,"metadata":{"id":"W5xc12xMsm0g"},"outputs":[],"source":"generate_caption(img_dt, np.random.choice(imgs))"},{"cell_type":"markdown","metadata":{"id":"TtYcGOA1RLXG"},"source":"The model is trained."},{"cell_type":"code","execution_count":null,"metadata":{"id":"il1FIrSAfxr6"},"outputs":[],"source":"# Fit the model\ncaption_model.fit(\n    train_dataset,\n    epochs=EPOCHS,\n    validation_data=valid_dataset,\n    callbacks=[early_stopping],\n)"},{"cell_type":"markdown","metadata":{"id":"Oc9D-Zk3RXex"},"source":"Generate captions for all test images."},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hh1npqFssYIF"},"outputs":[],"source":"for i in imgs:\n  generate_caption(img_dt, i)"},{"cell_type":"markdown","metadata":{"id":"P1BLZQJcRb9K"},"source":"Save the weights of the trained model."},{"cell_type":"code","execution_count":null,"metadata":{"id":"HHXKsQ0dfG8H"},"outputs":[],"source":"caption_model.save_weights(f'{tmpx}imgcap_{mdx}')"},{"cell_type":"markdown","metadata":{"id":"73ECgKXcRjsZ"},"source":"Dump the vectorised vocabulary."},{"cell_type":"code","execution_count":null,"metadata":{"id":"6fugZzf9vjH9"},"outputs":[],"source":"pickle.dump(vocab, open(f'/results/Vocab/{mdx}/vocab_{mdx}', 'wb'))"}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}