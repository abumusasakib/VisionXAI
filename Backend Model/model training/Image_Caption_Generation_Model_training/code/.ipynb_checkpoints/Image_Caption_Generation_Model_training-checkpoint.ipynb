{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVcpHBC7IgqH"
   },
   "source": [
    "# Preliminary settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HF4SBBAFIsOB"
   },
   "source": [
    "Libraries imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nxWhGVY-EN5v"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import efficientnet\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "from datetime import datetime\n",
    "seed = int(round(datetime.now().timestamp()))\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSIzi06ZJFbP"
   },
   "source": [
    "Path to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZMofSLZmE2RB"
   },
   "outputs": [],
   "source": [
    "dx = \"/data/train/rxxch9vw59.2/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWc1185pJOS9"
   },
   "source": [
    "Important constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TDp1iVL5EN5x"
   },
   "outputs": [],
   "source": [
    "# Path to the images\n",
    "IMAGES_PATH = dx+\"images\"\n",
    "\n",
    "# Desired image dimensions\n",
    "IMAGE_SIZE = (299, 299)\n",
    "\n",
    "# Vocabulary size\n",
    "VOCAB_SIZE = 1661\n",
    "# VOCAB_SIZE = len(final_vocab)\n",
    "\n",
    "# Fixed length allowed for any sequence\n",
    "SEQ_LENGTH = 15\n",
    "\n",
    "# Dimension for the image embeddings and token embeddings\n",
    "EMBED_DIM = 512\n",
    "\n",
    "# Per-layer units in the feed-forward network\n",
    "FF_DIM = 512\n",
    "\n",
    "# Other training parameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWXtvf3cJxTI"
   },
   "source": [
    "# Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZFNKop3SEKA"
   },
   "source": [
    "The image files are loaded. Each image is paired with two captions.\n",
    "The pairs are shuffled and split into 20% test and 80% train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "do7RDmN_EN5x",
    "outputId": "52dd409a-4830-43bd-c303-e24f8660da50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  7323\n",
      "Number of validation samples:  1831\n"
     ]
    }
   ],
   "source": [
    "def load_captions_data(filename):\n",
    "    \"\"\"Loads captions (text) data and maps them to corresponding images.\n",
    "\n",
    "    Args:\n",
    "        filename: Path to the text file containing caption data.\n",
    "\n",
    "    Returns:\n",
    "        caption_mapping: Dictionary mapping image names and the corresponding captions\n",
    "        text_data: List containing all the available captions\n",
    "    \"\"\"\n",
    "    with open(filename, encoding=\"utf8\") as caption_file:\n",
    "        caption_data = json.load(caption_file)\n",
    "        caption_mapping = {}\n",
    "        text_data = []\n",
    "\n",
    "        for item in caption_data:  # Iterate over the list of dictionaries\n",
    "            img_name = os.path.join(IMAGES_PATH, item['filename'].strip())  # Access 'filename'\n",
    "            caption_mapping[img_name] = [\"<start> \" + caption.strip() + \" <end>\" for caption in item['caption']]  # Access 'caption'\n",
    "            text_data.extend(caption_mapping[img_name])\n",
    "\n",
    "        return caption_mapping, text_data\n",
    "\n",
    "\n",
    "def train_val_split(caption_data, train_size=0.8, shuffle=True):\n",
    "    \"\"\"Split the captioning dataset into train and validation sets.\n",
    "\n",
    "    Args:\n",
    "        caption_data (dict): Dictionary containing the mapped caption data\n",
    "        train_size (float): Fraction of all the full dataset to use as training data\n",
    "        shuffle (bool): Whether to shuffle the dataset before splitting\n",
    "\n",
    "    Returns:\n",
    "        Training and validation datasets as two separated dicts\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Get the list of all image names\n",
    "    all_images = list(caption_data.keys())\n",
    "\n",
    "    # 2. Shuffle if necessary\n",
    "    if shuffle:\n",
    "        np.random.shuffle(all_images)\n",
    "\n",
    "    # 3. Split into training and validation sets\n",
    "    train_size = int(len(caption_data) * train_size)\n",
    "\n",
    "    training_data = {\n",
    "        img_name: caption_data[img_name] for img_name in all_images[:train_size]\n",
    "    }\n",
    "    validation_data = {\n",
    "        img_name: caption_data[img_name] for img_name in all_images[train_size:]\n",
    "    }\n",
    "\n",
    "    # 4. Return the splits\n",
    "    return training_data, validation_data\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "captions_mapping, text_data = load_captions_data(dx + \"captions.json\")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_data, valid_data = train_val_split(captions_mapping)\n",
    "print(\"Number of training samples: \", len(train_data))\n",
    "print(\"Number of validation samples: \", len(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('আছে।', 9264),\n",
       " ('একজন', 8775),\n",
       " ('একটি', 6006),\n",
       " ('পুরুষ', 4564),\n",
       " ('মানুষ', 4091),\n",
       " ('দাড়িয়ে', 3489),\n",
       " ('বসে', 3379),\n",
       " ('ও', 2350),\n",
       " ('দিয়ে', 2223),\n",
       " ('জন', 2201)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from collections import Counter\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the captions data from the file\n",
    "captions_path = dx + \"captions.json\"\n",
    "with open(captions_path, 'r', encoding='utf-8') as f:\n",
    "    captions_data = json.load(f)\n",
    "\n",
    "# Flatten all captions into a single list\n",
    "captions = []\n",
    "for item in captions_data:\n",
    "    captions.extend(item['caption'])\n",
    "\n",
    "# Tokenize the captions by splitting on spaces\n",
    "all_words = ' '.join(captions).split()\n",
    "\n",
    "# Count the frequency of each word\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# Visualize the word frequency distribution\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(list(word_freq.values()), bins=50)\n",
    "plt.yscale('log')\n",
    "plt.title('Word Frequency Distribution')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Return the word frequencies\n",
    "word_freq.most_common(10)  # Show the 10 most common words for reference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frequent words: 1659\n"
     ]
    }
   ],
   "source": [
    "min_word_freq = 5  # Threshold\n",
    "frequent_words = {word: count for word, count in word_freq.items() if count >= min_word_freq}\n",
    "\n",
    "# Check how many words have left after filtering\n",
    "print(f\"Number of frequent words: {len(frequent_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> তিন জন মেয়ে মানুষ আছে। এক জন দাড়িয়ে আছে আর দুই জন বসে আছে। <end>',\n",
       " '<start> একটি হলুদ জামা পায়জামা পরা মহিলা দাড়িয়ে হাতে একটি বেত নিয়ে পিটানোর ভাব দেখাচ্ছে আর ছোট একটি মেয়ে পিছনে ব্যাগ নিয়ে বসে কাঁদছে। <end>',\n",
       " '<start> অনেক মেয়ে মানুষ বসে আছে। <end>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uq7RaW9-LC9b"
   },
   "source": [
    "Image augmentation is performed and the captions are vectorised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1UFMzUbMEN5z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Vocabulary Length: 1659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 14:36:18.029276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-30 14:36:18.114447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-30 14:36:18.115164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-30 14:36:18.117210: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-30 14:36:18.119109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-30 14:36:18.119714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-30 14:36:18.120047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-30 14:36:19.589575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-30 14:36:19.590152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-30 14:36:19.590165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1609] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-11-30 14:36:19.590689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-30 14:36:19.591805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1386 MB memory:  -> device: 0, name: NVIDIA GeForce MX350, pci bus id: 0000:02:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary saved with 1661 tokens.\n"
     ]
    }
   ],
   "source": [
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "\n",
    "strip_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~\"\n",
    "strip_chars = strip_chars.replace(\"<\", \"\")\n",
    "strip_chars = strip_chars.replace(\">\", \"\")\n",
    "\n",
    "# List of words to include in the final vocabulary\n",
    "final_vocab = list(frequent_words.keys())\n",
    "print(f\"Final Vocabulary Length: {len(final_vocab)}\")\n",
    "\n",
    "# Initialize the TextVectorization layer with the final vocabulary\n",
    "vectorization = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=SEQ_LENGTH,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "vectorization.set_vocabulary(final_vocab)\n",
    "\n",
    "# Adapting the vectorization layer\n",
    "# vectorization.adapt(text_data)\n",
    "\n",
    "# Save the vocabulary to a JSON file after training\n",
    "vocab = vectorization.get_vocabulary()\n",
    "with open('/results/vocab.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(vocab, f, ensure_ascii=False, indent=4)\n",
    "print(f\"Vocabulary saved with {len(vocab)} tokens.\")\n",
    "\n",
    "# Data augmentation for image data\n",
    "image_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.2),\n",
    "        layers.RandomContrast(0.3),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 95.69%\n"
     ]
    }
   ],
   "source": [
    "covered = sum(word_freq[word] for word in final_vocab)\n",
    "total = sum(word_freq.values())\n",
    "print(f\"Coverage: {covered / total * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to expand the dataset such that each image is paired with each caption separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dataset(caption_mapping):\n",
    "    \"\"\"Flattens the dataset by creating separate (image, caption) pairs for each caption of an image.\n",
    "\n",
    "    Args:\n",
    "        caption_mapping: Dictionary mapping image names to a list of captions.\n",
    "\n",
    "    Returns:\n",
    "        images: List of image paths (repeated for each caption).\n",
    "        captions: List of corresponding captions (each caption as a string).\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    captions = []\n",
    "\n",
    "    # For each image, repeat the image path for each caption and append to the lists\n",
    "    for img_path, caption_list in caption_mapping.items():\n",
    "        for caption in caption_list:\n",
    "            images.append(img_path)\n",
    "            captions.append(caption)\n",
    "\n",
    "    return images, captions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENuangz2LOaT"
   },
   "source": [
    "The train and test datasets are created using TensorFlow Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eOUvwcMeEN50",
    "outputId": "6ae87204-37bb-46d8-cef5-24ad77ef350b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training dataset... \n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "make_dataset() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 96\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Pass the list of images and the list of corresponding captions\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Create train and validation datasets\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating training dataset... \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 96\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mmake_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_captions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_CAPTIONS_PER_IMAGE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating validation dataset... \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m valid_dataset \u001b[38;5;241m=\u001b[39m make_dataset(valid_images, valid_captions, NUM_CAPTIONS_PER_IMAGE)\n",
      "\u001b[0;31mTypeError\u001b[0m: make_dataset() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# Constants\n",
    "# Number of captions per image to use\n",
    "NUM_CAPTIONS_PER_IMAGE = 2\n",
    "\n",
    "# Suppress PNG warnings using PIL\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"Preprocess image to suppress warnings.\"\"\"\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            img = img.convert(\"RGB\")  # Strip unnecessary metadata\n",
    "            img.save(image_path, \"PNG\", icc_profile=None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "# Decode, resize, and preprocess images\n",
    "def decode_and_resize(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMAGE_SIZE)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return img\n",
    "\n",
    "\n",
    "def process_input(img_path, caption):\n",
    "    img = decode_and_resize(img_path)\n",
    "    \n",
    "    # Vectorize the caption\n",
    "    captions_vectorized = vectorization(caption)\n",
    "    \n",
    "    return img, captions_vectorized\n",
    "\n",
    "# Function to group captions by image\n",
    "# Group captions by image\n",
    "def group_captions_by_image(images, captions):\n",
    "    grouped_data = defaultdict(list)\n",
    "    for img, cap in zip(images, captions):\n",
    "        grouped_data[img].append(cap)\n",
    "\n",
    "    grouped_images = []\n",
    "    grouped_captions = []\n",
    "\n",
    "    for img, caps in grouped_data.items():\n",
    "        grouped_images.append(img)\n",
    "        grouped_captions.append(\n",
    "            caps[:NUM_CAPTIONS_PER_IMAGE] + [\"<pad>\"] * (NUM_CAPTIONS_PER_IMAGE - len(caps))\n",
    "        )\n",
    "\n",
    "    print(f\"Grouped {len(grouped_images)} images with captions.\")\n",
    "    return grouped_images, grouped_captions\n",
    "\n",
    "# Process images and captions\n",
    "def process_input(img_path, captions):\n",
    "    img = decode_and_resize(img_path)\n",
    "    captions_vectorized = vectorization(captions)\n",
    "    return img, captions_vectorized\n",
    "\n",
    "# Function for dataset creation\n",
    "# Create a TensorFlow Dataset\n",
    "def make_dataset(images, captions, num_captions_per_image):\n",
    "    # Group captions by image\n",
    "    grouped_images, grouped_captions = group_captions_by_image(images, captions)\n",
    "\n",
    "    # Ensure all groups have a consistent number of captions (pad/truncate)\n",
    "    padded_captions = [\n",
    "        caps[:num_captions_per_image] + [\"<pad>\"] * (num_captions_per_image - len(caps))\n",
    "        if len(caps) < num_captions_per_image else caps[:num_captions_per_image]\n",
    "        for caps in grouped_captions\n",
    "    ]\n",
    "\n",
    "    # Create a TensorFlow Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((grouped_images, padded_captions))\n",
    "    dataset = dataset.shuffle(BATCH_SIZE * 8)\n",
    "\n",
    "    # Map images and captions to processed inputs\n",
    "    dataset = dataset.map(\n",
    "        lambda img, caps: (decode_and_resize(img), vectorization(caps)),\n",
    "        num_parallel_calls=AUTOTUNE\n",
    "    )\n",
    "\n",
    "    for img, cap in dataset.take(1):\n",
    "        print(f\"Image and grouped caption shape: {img.shape, cap.shape} \\n\")\n",
    "\n",
    "    # Batch and prefetch the dataset\n",
    "    dataset = dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# def make_dataset(images, captions):\n",
    "#     print([len(i) for i in images[:5]])\n",
    "#     print([len(i) for i in captions[:5]])\n",
    "\n",
    "# Dataset Preparation\n",
    "# Flatten the training data and create datasets\n",
    "train_images, train_captions = flatten_dataset(train_data)\n",
    "valid_images, valid_captions = flatten_dataset(valid_data)\n",
    "\n",
    "# Pass the list of images and the list of corresponding captions\n",
    "# Create train and validation datasets\n",
    "print(\"Creating training dataset... \\n\")\n",
    "train_dataset = make_dataset(train_images, train_captions, NUM_CAPTIONS_PER_IMAGE)\n",
    "\n",
    "print(\"Creating validation dataset... \\n\")\n",
    "valid_dataset = make_dataset(valid_images, valid_captions, NUM_CAPTIONS_PER_IMAGE)\n",
    "\n",
    "# Print the shapes of the datasets\n",
    "# Debugging shapes\n",
    "for img_batch, cap_batch in train_dataset.take(1):\n",
    "    print(f\"Training batch image shape: {img_batch.shape}, Caption shape: {cap_batch.shape}\")\n",
    "for img_batch, cap_batch in valid_dataset.take(1):\n",
    "    print(f\"Validation batch image shape: {img_batch.shape}, Caption shape: {cap_batch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dataset_statistics(captions):\n",
    "    \"\"\"Print statistics for the caption dataset.\"\"\"\n",
    "    caption_lengths = [len(cap.split()) for cap in captions]\n",
    "    print(f\"Total Captions: {len(captions)}\")\n",
    "    print(f\"Max Caption Length: {max(caption_lengths)}\")\n",
    "    print(f\"Min Caption Length: {min(caption_lengths)}\")\n",
    "    print(f\"Mean Caption Length: {sum(caption_lengths) / len(caption_lengths):.2f}\")\n",
    "\n",
    "# Compute statistics\n",
    "print(\"Training Dataset Statistics:\")\n",
    "compute_dataset_statistics(train_captions)\n",
    "\n",
    "print(\"Validation Dataset Statistics:\")\n",
    "compute_dataset_statistics(valid_captions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5j57Tz3Mzpi"
   },
   "source": [
    "# Transformer model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aMSR1TdSMJK"
   },
   "source": [
    "The Transformer uses the EfficientNetB0 CNN.\n",
    "The Encoder and Decoder block and Positional Embedding layer is seperately created by inheriting the Layer class.\n",
    "The Transformer model is created by compiling the layers and inheriting the Model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDNvmXFSEN50",
    "jupyter": {
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_cnn_model():\n",
    "    base_model = efficientnet.EfficientNetB0(\n",
    "        input_shape=(*IMAGE_SIZE, 3), include_top=False, weights=\"imagenet\",\n",
    "    )\n",
    "    # We freeze our feature extractor\n",
    "    base_model.trainable = False\n",
    "    base_model_out = base_model.output\n",
    "    \n",
    "    # Reduce the sequence length using a pooling operation\n",
    "    # Usign GlobalAveragePooling2D to reduce the spatial dimensions\n",
    "    base_model_out = layers.GlobalAveragePooling2D()(base_model_out)\n",
    "    \n",
    "    # Optionally, project the output to match the embedding size\n",
    "    base_model_out = layers.Dense(EMBED_DIM)(base_model_out)\n",
    "    \n",
    "    cnn_model = keras.models.Model(base_model.input, base_model_out)\n",
    "    \n",
    "    # Print CNN Model Summary\n",
    "    print(\"\\nCNN Model Summary:\")\n",
    "    cnn_model.summary()\n",
    "    return cnn_model\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.0\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization() # Potential error source\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.dense_1 = layers.Dense(embed_dim, activation=\"relu\")\n",
    "\n",
    "    def call(self, inputs, training, mask=None):\n",
    "        # Input shape\n",
    "        print(f\"Encoder Input Shape: {inputs.shape}\")\n",
    "\n",
    "        print(f\"Encoder Input Shape before LayerNorm: {inputs.shape}\")\n",
    "        inputs = self.layernorm_1(inputs)\n",
    "        print(f\"Encoder Input Shape after LayerNorm: {inputs.shape}\")\n",
    "        \n",
    "        inputs = self.dense_1(inputs)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=None,\n",
    "            training=training,\n",
    "        )\n",
    "        \n",
    "        out_1 = self.layernorm_2(inputs + attention_output_1)\n",
    "\n",
    "        # Output shape\n",
    "        print(f\"Encoder Output Shape: {out_1.shape}\")\n",
    "        return out_1\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        print(f\"Positional Embedding Input Shape: {inputs.shape}\")\n",
    "        \n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_tokens = embedded_tokens * self.embed_scale\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        \n",
    "        # Embeddings shape\n",
    "        print(f\"Positional Embedding Output Shape: {embedded_tokens.shape}\")\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class TransformerDecoderBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ff_dim = ff_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
    "        )\n",
    "        self.ffn_layer_1 = layers.Dense(ff_dim, activation=\"relu\")\n",
    "        self.ffn_layer_2 = layers.Dense(embed_dim)\n",
    "\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "\n",
    "        self.embedding = PositionalEmbedding(\n",
    "            embed_dim=EMBED_DIM, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE\n",
    "        )\n",
    "        self.out = layers.Dense(VOCAB_SIZE, activation=\"softmax\")\n",
    "\n",
    "        self.dropout_1 = layers.Dropout(0.3)\n",
    "        self.dropout_2 = layers.Dropout(0.5)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, training, mask=None):\n",
    "        print(f\"Decoder Input Shape: {inputs.shape}\")\n",
    "        \n",
    "        inputs = self.embedding(inputs)\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n",
    "            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
    "            combined_mask = tf.minimum(combined_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=combined_mask,\n",
    "            training=training,\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "            training=training,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        ffn_out = self.ffn_layer_1(out_2)\n",
    "        ffn_out = self.dropout_1(ffn_out, training=training)\n",
    "        ffn_out = self.ffn_layer_2(ffn_out)\n",
    "\n",
    "        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)\n",
    "        ffn_out = self.dropout_2(ffn_out, training=training)\n",
    "        preds = self.out(ffn_out)\n",
    "        \n",
    "        print(f\"Decoder Output Shape: {preds.shape}\")\n",
    "        return preds\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "\n",
    "class ImageCaptioningModel(keras.Model):\n",
    "    def __init__(\n",
    "        self, cnn_model, encoder, decoder, num_captions_per_image=2, image_aug=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cnn_model = cnn_model\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "        self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n",
    "        self.num_captions_per_image = num_captions_per_image\n",
    "        self.image_aug = image_aug\n",
    "\n",
    "    def calculate_loss(self, y_true, y_pred, mask):\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "        mask = tf.cast(mask, dtype=loss.dtype)\n",
    "        loss *= mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "    def calculate_accuracy(self, y_true, y_pred, mask):\n",
    "        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
    "        accuracy = tf.math.logical_and(mask, accuracy)\n",
    "        accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
    "\n",
    "    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):\n",
    "        # print(f\"Image Embedding Input Shape before passing to Encoder: {img_embed.shape}\")\n",
    "        \n",
    "        # batch_seq = tf.expand_dims(batch_seq, axis=1)\n",
    "        print(f\"Batch Sequence Input Shape before slicing: {batch_seq.shape}\")\n",
    "        \n",
    "        encoder_out = self.encoder(img_embed, training=training)\n",
    "        batch_seq_inp = batch_seq[:, :-1] # Input sequence (without the last token)\n",
    "\n",
    "        # print(f\"Batch Sequence Input Shape before target sequence: {batch_seq_inp.shape}\")\n",
    "        \n",
    "        batch_seq_true = batch_seq[:, 1:] # Target sequence (without the first token)\n",
    "        mask = tf.math.not_equal(batch_seq_true, 0)\n",
    "        \n",
    "        print(f\"Batch Sequence Input Shape: {batch_seq_inp.shape}\")\n",
    "        print(f\"Batch Sequence True Shape: {batch_seq_true.shape}\")\n",
    "        \n",
    "        batch_seq_pred = self.decoder(\n",
    "            batch_seq_inp, encoder_out, training=training, mask=mask\n",
    "        )\n",
    "\n",
    "        print(f\"Batch Sequence Predicted Shape: {batch_seq_pred.shape}\")\n",
    "        \n",
    "        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)\n",
    "        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)\n",
    "        return loss, acc\n",
    "\n",
    "    def train_step(self, batch_data):\n",
    "        batch_img, batch_seq = batch_data\n",
    "\n",
    "        # batch_seq = tf.expand_dims(batch_seq, axis=1)\n",
    "\n",
    "        # print(f\"Training Image Batch Shape before passing to CNN: {batch_img.shape}\")\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "    \n",
    "        if self.image_aug:\n",
    "            batch_img = self.image_aug(batch_img)\n",
    "\n",
    "        print(f\"Training Image Batch Shape: {batch_img.shape}\")\n",
    "        print(f\"Training Sequence Batch Shape: {batch_seq.shape}\")\n",
    "        \n",
    "        # 1. Get image embeddings from CNN\n",
    "        img_embed = self.cnn_model(batch_img)\n",
    "        print(f\"Image Embeddings Shape: {img_embed.shape}\")\n",
    "\n",
    "        # 2. Reshape CNN output to (batch_size, 1, embedding_dim)\n",
    "        img_embed = tf.expand_dims(img_embed, axis=1)  # It should be (None, 1, 1024)\n",
    "\n",
    "        print(f\"Reshaped Image Embeddings for Encoder: {img_embed.shape}\")\n",
    "        \n",
    "        # 3. Make sure batch_seq has 3 dimensions\n",
    "        if batch_seq.shape.ndims == 2:\n",
    "            # Reshape the sequence to have a third dimension (e.g., 1 caption per image)\n",
    "            batch_seq = tf.expand_dims(batch_seq, axis=1)\n",
    "        \n",
    "        print(f\"Updated Sequence Shape: {batch_seq.shape}\")\n",
    "\n",
    "        # 4. Accumulate loss and accuracy for each caption\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Loop through each caption (batch_seq should be (batch_size, num_captions, sequence_length))\n",
    "            num_captions_per_image = batch_seq.shape[1] # Extract the num_captions dimension\n",
    "            \n",
    "            for i in range(num_captions_per_image):\n",
    "                loss, acc = self._compute_caption_loss_and_acc(\n",
    "                    img_embed, batch_seq[:, i, :], training=True\n",
    "                )\n",
    "                total_loss += loss\n",
    "                total_acc += acc\n",
    "\n",
    "            # 5. Compute the mean loss and accuracy\n",
    "            avg_loss = total_loss / tf.cast(num_captions_per_image, dtype=tf.float32)\n",
    "            avg_acc = total_acc / tf.cast(num_captions_per_image, dtype=tf.float32)\n",
    "\n",
    "        # Backpropagation\n",
    "        # 6. Get the list of all the trainable weights\n",
    "        train_vars = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "        \n",
    "        # 7. Get the gradients (from the accumulated loss)\n",
    "        grads = tape.gradient(avg_loss, train_vars)\n",
    "    \n",
    "        # 8. Update the trainable weights\n",
    "        self.optimizer.apply_gradients(zip(grads, train_vars))\n",
    "    \n",
    "        # 9. Update the trackers\n",
    "        self.loss_tracker.update_state(avg_loss)\n",
    "        self.acc_tracker.update_state(avg_acc)\n",
    "    \n",
    "        # 10. Return the loss and accuracy values\n",
    "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
    "\n",
    "    def test_step(self, batch_data):\n",
    "        batch_img, batch_seq = batch_data\n",
    "\n",
    "        # batch_seq = tf.expand_dims(batch_seq, axis=1)\n",
    "\n",
    "        batch_loss = 0\n",
    "        batch_acc = 0\n",
    "\n",
    "        # 1. Get image embeddings\n",
    "        img_embed = self.cnn_model(batch_img)\n",
    "\n",
    "        # 2. Pass each of the captions one by one to the decoder\n",
    "        # along with the encoder outputs and compute the loss as well as accuracy\n",
    "        # for each caption.\n",
    "        for i in range(self.num_captions_per_image):\n",
    "            loss, acc = self._compute_caption_loss_and_acc(\n",
    "                img_embed, batch_seq[:, i, :], training=False\n",
    "            )\n",
    "\n",
    "            # 3. Update batch loss and batch accuracy\n",
    "            batch_loss += loss\n",
    "            batch_acc += acc\n",
    "\n",
    "        batch_acc /= float(self.num_captions_per_image)\n",
    "\n",
    "        # 4. Update the trackers\n",
    "        self.loss_tracker.update_state(batch_loss)\n",
    "        self.acc_tracker.update_state(batch_acc)\n",
    "\n",
    "        # 5. Return the loss and accuracy values\n",
    "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics here so the `reset_states()` can be\n",
    "        # called automatically.\n",
    "        return [self.loss_tracker, self.acc_tracker]\n",
    "\n",
    "\n",
    "cnn_model = get_cnn_model()\n",
    "encoder = TransformerEncoderBlock(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=1)\n",
    "decoder = TransformerDecoderBlock(embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=2)\n",
    "caption_model = ImageCaptioningModel(\n",
    "    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hV7XUwZIPLKa"
   },
   "source": [
    "The loss function and early stopping is defined. The model is compiled with the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1vztqwkOEN51"
   },
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "cross_entropy = keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False, reduction=\"none\"\n",
    ")\n",
    "\n",
    "# EarlyStopping criteria\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# Learning Rate Scheduler for the optimizer\n",
    "class LRSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, post_warmup_learning_rate, warmup_steps):\n",
    "        super().__init__()\n",
    "        self.post_warmup_learning_rate = post_warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        global_step = tf.cast(step, tf.float32)\n",
    "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
    "        warmup_progress = global_step / warmup_steps\n",
    "        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress\n",
    "        return tf.cond(\n",
    "            global_step < warmup_steps,\n",
    "            lambda: warmup_learning_rate,\n",
    "            lambda: self.post_warmup_learning_rate,\n",
    "        )\n",
    "\n",
    "\n",
    "# Create a learning rate schedule\n",
    "num_train_steps = len(train_dataset) * EPOCHS\n",
    "num_warmup_steps = num_train_steps // 15\n",
    "lr_schedule = LRSchedule(post_warmup_learning_rate=1e-4, warmup_steps=num_warmup_steps)\n",
    "\n",
    "# Compile the model\n",
    "caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HklGIq3dPi2W"
   },
   "source": [
    "# Model training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmJ0CodqPwQJ"
   },
   "source": [
    "Sets the version and checks if the model has been previously trained or being trained for the first time. If saved weights are found, they're loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NuvkTjn3d9nC"
   },
   "outputs": [],
   "source": [
    "mdx = '231005'  # Sets the version\n",
    "tmpx = f'/results/Model_weights/{mdx}/Temp/'\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(tmpx):\n",
    "    print(f\"Directory {tmpx} does not exist. Creating the directory.\")\n",
    "    os.makedirs(tmpx)  # Create the directory if it doesn't exist\n",
    "\n",
    "# Now check for the files\n",
    "try:\n",
    "    weight_path = f'{tmpx}imgcap_{mdx}'\n",
    "    fls = os.listdir(tmpx)\n",
    "\n",
    "    # Look for specific weight files (like .index or .data-00000-of-00001)\n",
    "    checkpoint_files = [f for f in fls if \"imgcap_\" in f]\n",
    "    \n",
    "    if len(checkpoint_files) > 0:\n",
    "        print(\"Found saved weights, loading them now...\")\n",
    "        caption_model.load_weights(weight_path)\n",
    "        print(\"Saved weights loaded successfully\")\n",
    "    else:\n",
    "        print(\"No saved weights found, training from scratch\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGNoQWakQPVn"
   },
   "source": [
    "Lookup dictionary created and output sequence length is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Jv4pzQqEN51"
   },
   "outputs": [],
   "source": [
    "# Load the vocabulary during inference\n",
    "with open('/results/vocab.json', 'r', encoding='utf-8') as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "# Recreate the vectorization object and set its vocabulary\n",
    "vectorization = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=SEQ_LENGTH,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "\n",
    "# Set the vocabulary during inference\n",
    "vectorization.set_vocabulary(vocab)\n",
    "\n",
    "print(f\"Vocabulary loaded with {len(vocab)} tokens.\")\n",
    "\n",
    "index_lookup = dict(zip(range(len(vocab)), vocab))\n",
    "max_decoded_sentence_length = SEQ_LENGTH - 1\n",
    "valid_images = list(valid_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFil9RZiQf5F"
   },
   "source": [
    "The function reads an image from the given path. It uses the image to generate an caption. The same are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SFfLGgVEmzHb",
    "jupyter": {
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Handle invalid token indices\n",
    "def generate_caption(dt, ix):\n",
    "    # Select a random image from the validation dataset\n",
    "    sample_img = f'{dt}images/{ix}'\n",
    "\n",
    "    # Read the image from the disk\n",
    "    sample_img = decode_and_resize(sample_img)\n",
    "    img = sample_img.numpy().clip(0, 255).astype(np.uint8)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "    # Pass the image to the CNN\n",
    "    img = tf.expand_dims(sample_img, 0)\n",
    "    img = caption_model.cnn_model(img)\n",
    "\n",
    "    # Expand dimensions to make it compatible with the encoder\n",
    "    img = tf.expand_dims(img, 1)  # Adding sequence dimension, shape becomes (batch_size, 1, embed_dim)\n",
    "\n",
    "    # Pass the image features to the Transformer encoder\n",
    "    encoded_img = caption_model.encoder(img, training=False)\n",
    "\n",
    "    # Generate the caption using the Transformer decoder\n",
    "    decoded_caption = \"<start> \"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        # Ensure decoded_caption is passed as a list of strings\n",
    "        tokenized_caption = vectorization(tf.constant([decoded_caption]))[:, :-1]\n",
    "        \n",
    "        # Create mask for the tokenized caption\n",
    "        mask = tf.math.not_equal(tokenized_caption, 0)\n",
    "\n",
    "        predictions = caption_model.decoder(\n",
    "            tokenized_caption, encoded_img, training=False, mask=mask\n",
    "        )\n",
    "\n",
    "        # Get the predicted token\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "\n",
    "        # Check if sampled_token_index is in the vocabulary range\n",
    "        if sampled_token_index >= len(vocab):\n",
    "            print(f\"Warning: Token index {sampled_token_index} out of range\")\n",
    "            continue  # Skip to the next iteration if the token is out of range\n",
    "\n",
    "        sampled_token = index_lookup[sampled_token_index]\n",
    "\n",
    "        if sampled_token == \"<end>\":\n",
    "            break\n",
    "\n",
    "        decoded_caption += \" \" + sampled_token\n",
    "\n",
    "    # Clean up the decoded caption\n",
    "    decoded_caption = decoded_caption.replace(\"<start> \", \"\")\n",
    "    decoded_caption = decoded_caption.replace(\" <end>\", \"\").strip()\n",
    "    print(\"\\nPredicted Caption: \", decoded_caption)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b49GQfkuQwx6"
   },
   "source": [
    "Test image path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1_T-wHXnbHw"
   },
   "outputs": [],
   "source": [
    "img_dt = \"/data/test/rxxch9vw59-2/\"\n",
    "imgs = os.listdir(img_dt+\"images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_hiYHq-Q3f-"
   },
   "source": [
    "Generates a caption for a random image among test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_image = np.random.choice(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W5xc12xMsm0g"
   },
   "outputs": [],
   "source": [
    "# generate_caption(img_dt,random_image)\n",
    "generate_caption(img_dt,'1228.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtYcGOA1RLXG"
   },
   "source": [
    "The model is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "il1FIrSAfxr6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define checkpoint callback\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath=f'{tmpx}imgcap_{mdx}', \n",
    "    save_weights_only=True, \n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Fit the model\n",
    "    caption_model.fit(\n",
    "        train_dataset,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=valid_dataset,\n",
    "        callbacks=[early_stopping, checkpoint_cb],\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"ValueError during model fitting: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oc9D-Zk3RXex"
   },
   "source": [
    "Generate captions for all test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hh1npqFssYIF"
   },
   "outputs": [],
   "source": [
    "for i in imgs:\n",
    "  generate_caption(img_dt, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1BLZQJcRb9K"
   },
   "source": [
    "Save the weights of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHXKsQ0dfG8H"
   },
   "outputs": [],
   "source": [
    "# After training the model, save the weights\n",
    "caption_model.save_weights(f'{tmpx}imgcap_{mdx}', save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73ECgKXcRjsZ"
   },
   "source": [
    "Dump the vectorised vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fugZzf9vjH9"
   },
   "outputs": [],
   "source": [
    "# Define the directory path\n",
    "directory = f'/results/Vocab/{mdx}'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the vocabulary using pickle\n",
    "with open(f'{directory}/vocab_{mdx}', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
